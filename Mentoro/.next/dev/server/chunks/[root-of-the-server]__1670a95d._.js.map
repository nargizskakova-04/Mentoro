{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 46, "column": 0}, "map": {"version":3,"sources":["file:///Users/dilyarabaizova/Mentoro/Mentoro/src/app/api/quizzes/generate/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\n\nconst MAX_CONTEXT_CHARS = 6000;\n\nfunction truncateForContext(text: string): string {\n    if (text.length <= MAX_CONTEXT_CHARS) return text;\n    return text.slice(0, MAX_CONTEXT_CHARS) + '\\n\\n[... обрезано ...]';\n}\n\nconst genAI = new GoogleGenerativeAI(\"AIzaSyBahf8utHGsO8sT7VrtHMGDbvQ8Qwr7Yc4\");\n\nexport async function POST(request: NextRequest) {\n    try {\n        const body = await request.json();\n        const { type, documentText } = body; // 'explain' or 'quiz'\n\n        if (!type || !['explain', 'quiz'].includes(type)) {\n            return NextResponse.json({ error: 'Invalid generation type' }, { status: 400 });\n        }\n\n        let contextText = '';\n        if (documentText && typeof documentText === 'string' && documentText.trim()) {\n            contextText = documentText.trim();\n        } else {\n            const query = type === 'explain' ? 'summary overview main points' : 'test questions key facts';\n            const relevantChunks = await retrieveRelevantChunks(query, 5);\n            contextText = relevantChunks.map((c) => c.text).join('\\n---\\n');\n        }\n\n        if (!contextText) {\n            return NextResponse.json(\n                { error: 'No document content available. Please upload a document first.' },\n                { status: 400 }\n            );\n        }\n\n        contextText = truncateForContext(contextText);\n\n        let systemPrompt = '';\n        let userPrompt = '';\n\n        if (type === 'explain') {\n            systemPrompt = `You are a study assistant. Your task is to explain and help people learn. Study the received data and explain what it is about. Also answer follow-up questions from the user. Use Markdown formatting for readability.`;\n            userPrompt = `Study the following document and explain what it is about:\\n\\n${contextText}`;\n        } else {\n            systemPrompt = `You are a strict output generator. You must generate a JSON array of 15-20 multiple choice questions.\n\nIMPORTANT: Questions must be about the SUBJECT MATTER and LEARNING CONTENT inside the document - test the student's understanding of the concepts, definitions, facts, and ideas presented in the text. Do NOT ask meta-questions about the document itself (e.g. \"What format is this document?\" or \"How many sections does this have?\").\n\nThe output must be a valid JSON array of objects. NO markdown, NO code blocks, just raw JSON.\nEach object must have:\n- \"question\": string (tests knowledge from the content)\n- \"options\": array of 4 strings\n- \"correctAnswer\": string (must match one of the options exactly)\n- \"explanation\": string (brief explanation referring to the document content)\n\nExample format:\n[\n    {\"question\": \"According to the text, what is X?\", \"options\": [\"A\", \"B\", \"C\", \"D\"], \"correctAnswer\": \"B\", \"explanation\": \"The document states...\"},\n    {\"question\": \"Which concept is defined as...?\", \"options\": [\"...\", \"...\", \"...\", \"...\"], \"correctAnswer\": \"...\", \"explanation\": \"...\"}\n]`;\n            userPrompt = `Generate 15-20 multiple choice questions that test understanding of the material in this document:\\n\\n${contextText}`;\n        }\n\n        const response = await openai.chat.completions.create({\n            model: 'gemini-1.5-flash', // Или gemini-1.5-pro для более сложных задач\n            messages: [\n                { role: 'system', content: systemPrompt },\n                { role: 'user', content: userPrompt },\n            ],\n            stream: true,\n            // Для генерации квиза (JSON) Gemini лучше работает с этим параметром:\n            response_format: type === 'quiz' ? { type: 'json_object' } : undefined,\n        });\n\n        const stream = new ReadableStream({\n            async start(controller) {\n                try {\n                    for await (const chunk of response) {\n                        const content = chunk.choices[0]?.delta?.content || '';\n                        if (content) {\n                            controller.enqueue(new TextEncoder().encode(content));\n                        }\n                    }\n                } catch (streamError: unknown) {\n                    const err = streamError instanceof Error ? streamError : new Error(String(streamError));\n                    const causeMsg = err.cause instanceof Error ? err.cause.message : '';\n                    const msg = err.message + ' ' + causeMsg;\n                    if (msg.includes('Context size') || msg.includes('context') || msg.includes('exceeded')) {\n                        controller.enqueue(new TextEncoder().encode('\\n\\n⚠️ Документ слишком большой для модели. Попробуйте загрузить файл меньшего размера или использовать более короткий текст.'));\n                    } else {\n                        controller.enqueue(new TextEncoder().encode('\\n\\n⚠️ Ошибка при генерации. Попробуйте ещё раз.'));\n                    }\n                } finally {\n                    controller.close();\n                }\n            },\n        });\n\n        return new NextResponse(stream, {\n            headers: {\n                'Content-Type': 'text/plain',\n                'Transfer-Encoding': 'chunked',\n            },\n        });\n\n    } catch (error) {\n        console.error('Generation error:', error);\n        return NextResponse.json({ error: 'Failed to generate content' }, { status: 500 });\n    }\n}\n"],"names":[],"mappings":";;;;AAAA;AACA;;;AAGA,MAAM,oBAAoB;AAE1B,SAAS,mBAAmB,IAAY;IACpC,IAAI,KAAK,MAAM,IAAI,mBAAmB,OAAO;IAC7C,OAAO,KAAK,KAAK,CAAC,GAAG,qBAAqB;AAC9C;AAEA,MAAM,QAAQ,IAAI,sLAAkB,CAAC;AAE9B,eAAe,KAAK,OAAoB;IAC3C,IAAI;QACA,MAAM,OAAO,MAAM,QAAQ,IAAI;QAC/B,MAAM,EAAE,IAAI,EAAE,YAAY,EAAE,GAAG,MAAM,sBAAsB;QAE3D,IAAI,CAAC,QAAQ,CAAC;YAAC;YAAW;SAAO,CAAC,QAAQ,CAAC,OAAO;YAC9C,OAAO,gJAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAA0B,GAAG;gBAAE,QAAQ;YAAI;QACjF;QAEA,IAAI,cAAc;QAClB,IAAI,gBAAgB,OAAO,iBAAiB,YAAY,aAAa,IAAI,IAAI;YACzE,cAAc,aAAa,IAAI;QACnC,OAAO;YACH,MAAM,QAAQ,SAAS,YAAY,iCAAiC;YACpE,MAAM,iBAAiB,MAAM,uBAAuB,OAAO;YAC3D,cAAc,eAAe,GAAG,CAAC,CAAC,IAAM,EAAE,IAAI,EAAE,IAAI,CAAC;QACzD;QAEA,IAAI,CAAC,aAAa;YACd,OAAO,gJAAY,CAAC,IAAI,CACpB;gBAAE,OAAO;YAAiE,GAC1E;gBAAE,QAAQ;YAAI;QAEtB;QAEA,cAAc,mBAAmB;QAEjC,IAAI,eAAe;QACnB,IAAI,aAAa;QAEjB,IAAI,SAAS,WAAW;YACpB,eAAe,CAAC,uNAAuN,CAAC;YACxO,aAAa,CAAC,8DAA8D,EAAE,aAAa;QAC/F,OAAO;YACH,eAAe,CAAC;;;;;;;;;;;;;;;CAe3B,CAAC;YACU,aAAa,CAAC,sGAAsG,EAAE,aAAa;QACvI;QAEA,MAAM,WAAW,MAAM,OAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC;YAClD,OAAO;YACP,UAAU;gBACN;oBAAE,MAAM;oBAAU,SAAS;gBAAa;gBACxC;oBAAE,MAAM;oBAAQ,SAAS;gBAAW;aACvC;YACD,QAAQ;YACR,sEAAsE;YACtE,iBAAiB,SAAS,SAAS;gBAAE,MAAM;YAAc,IAAI;QACjE;QAEA,MAAM,SAAS,IAAI,eAAe;YAC9B,MAAM,OAAM,UAAU;gBAClB,IAAI;oBACA,WAAW,MAAM,SAAS,SAAU;wBAChC,MAAM,UAAU,MAAM,OAAO,CAAC,EAAE,EAAE,OAAO,WAAW;wBACpD,IAAI,SAAS;4BACT,WAAW,OAAO,CAAC,IAAI,cAAc,MAAM,CAAC;wBAChD;oBACJ;gBACJ,EAAE,OAAO,aAAsB;oBAC3B,MAAM,MAAM,uBAAuB,QAAQ,cAAc,IAAI,MAAM,OAAO;oBAC1E,MAAM,WAAW,IAAI,KAAK,YAAY,QAAQ,IAAI,KAAK,CAAC,OAAO,GAAG;oBAClE,MAAM,MAAM,IAAI,OAAO,GAAG,MAAM;oBAChC,IAAI,IAAI,QAAQ,CAAC,mBAAmB,IAAI,QAAQ,CAAC,cAAc,IAAI,QAAQ,CAAC,aAAa;wBACrF,WAAW,OAAO,CAAC,IAAI,cAAc,MAAM,CAAC;oBAChD,OAAO;wBACH,WAAW,OAAO,CAAC,IAAI,cAAc,MAAM,CAAC;oBAChD;gBACJ,SAAU;oBACN,WAAW,KAAK;gBACpB;YACJ;QACJ;QAEA,OAAO,IAAI,gJAAY,CAAC,QAAQ;YAC5B,SAAS;gBACL,gBAAgB;gBAChB,qBAAqB;YACzB;QACJ;IAEJ,EAAE,OAAO,OAAO;QACZ,QAAQ,KAAK,CAAC,qBAAqB;QACnC,OAAO,gJAAY,CAAC,IAAI,CAAC;YAAE,OAAO;QAA6B,GAAG;YAAE,QAAQ;QAAI;IACpF;AACJ"}}]
}